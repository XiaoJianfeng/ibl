#!/novo/users/jfgx/local/bin/python
import sys
import re
import gzip
import sqlite3
import os.path as path
from os import makedirs
from subprocess import check_output
from multiprocessing import Process
from itertools import islice, izip

"""
function: fastx toolkit couldn't do trimming or filtering on paired-end reads,
          so we can use fastx to do trimming, clipping or filtering on separate
          file, and then use this script to extract fastq records common in both files.
usage: intersect_fastqsqlite3_guess.py [-h]
                                       [-f {guess,infer,custom,illumina,AROS,illumina_old}]
                                       [-o output_dir] [-p prefix] [-F/--force_overwrite] [-z/--gzip]
                                       [-m/--multiprocessing]
                                       fq_file fq_file

To get intersection of a pair of fastq files that have been filtered
separately.

positional arguments:
  fq_file               fastq1 and fastq2 (could be gzip file)

optional arguments:
  -h, --help            show this help message and exit
  -f {guess,infer,custom,illumina,AROS,illumina_old}, --header_format {guess,infer,custom,illumina,AROS,illumina_old}
                        header line format of the fastq file, default is
                        'guess'
  -o output_dir, --output_dir output_dir
                        directory of the output files
  -p prefix, --prefix prefix
                        prefix of the output files. If prefix is relative
                        path, it will be joined to output_dir; If prefix is
                        absolute path, output_dir will be ignored.
  -F, --force_overwrite
                        force overwrite if output file already exists. Default
                        is 'False'.
  -z, --gzip            output file in gzip format. Default is 'True'.
  -m, --multi_processing
                        to use multiprocessing to speed up the writing of
                        output files. Default if 'False'. Note: 1) if you
                        submit job with PBS, you may need to assign ncpus=3
                        for every job; 2) multi_processing could only help if
                        the output is in .gzip format.

last modified:
    2014.7.9  -- add new format support for new illumin header format (the same with header2ID_AROS)
                 fix a small bug when the input fastq files are blank.
    2013.9.29 -- make --gzip default to True
    2013.1.23 -- rewrite fastq_iter to not use file_block_iter() to make the code more clear
    2013.1.22 -- make the script to guess header line format by default, as suggested by HIWT; clean up the code.
    2013.1.18 -- use Sqlite3 to find the intersection of two fastq files.
    2013.1.15 -- fix a bug that affects fastq files with AROS and custom header format.
    2013.1.8 -- add support for more fastq header format
                add support for gzip (both input and output)
    2012.8.2 -- use frozenset to replace set to improve efficiency
    2012.3.9 -- add a '\n' in sys.stderr.write
    2012.3.5 -- use idAb and idaB, instead of idAB which is much bigger in most cases.
"""

#-------------------------------------------------
DEBUG = False

stderr_write = sys.stderr.write
#-------------------------------------------------
def date():
    """ get output of `date` by calling shell"""

    return check_output("date", shell=True).rstrip()

#-------------------------------------------------
def fastq_iter(f, buffsize=int(1e5)):
    """
    fastq generator
    a fastq record is a list of 4 lines

    f:        a file name or a file-like object(should have .readline(); both generated by open() or gzip.open() will work)
    buffsize: number of lines to read in each chunk
    """

    if isinstance(f, basestring):
        if f.endswith(".gz"):
            fobj = gzip.open(f, 'rb')
        else:
            fobj = open(f, 'r')
    elif hasattr(f, "readline") and callable(f.readline):
        fobj = f
    else:
        raise Exception("Invalid file: %s" % f)

    while True:
        records = list(islice(fobj, buffsize))
        if records:
            n = len(records)
            if not n % 4:   # if len(records) == 0
                for i in range(0, n, 4):
                    yield records[i:i + 4]
            else:         # if len(records) != 0
                raise Exception("Error, the lines of the file should be 4*n!\n Error record:\n%s" % "".join(records[n // 4 * 4:]))
        else:
            break

#-------------------------------------------------
# NOTE: every time add new header format, should also add re patterns to be used for checking in guess_header_format()

def header2ID(header_line):
    """ To extract a uniq ID that could differentiate 1st and 2end read of the same pair from header line.

    input:
        header_line:  the header line of a fastq record

    Note: 1) reads from the same pair should have the same ID. 
          2) reads from the same file should have different ID.
          3) The ID will be stored in memory, so shorter ID will occupy less memory.
    """

    raise Exception("Function header2ID not implemented.")

def header2ID_illumina_old(header_line):
    """ To extract ID for Illumina fastq file.

    Note: for paired-end reads by illumina, the header lines for the same pair of reads are different at the end (/1, /2)
          header_line[:-3] is used to get the common part of the header line

    example:
        "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/1" --> "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/"
        "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/2" --> "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/"
    """

    # return header_line.split('#')[0]
    return header_line[:-3]
header2ID_illumina_old.p1 = re.compile(r'^@.+/1$')
header2ID_illumina_old.p2 = re.compile(r'^@.+/2$')

def header2ID_illumina(header_line):
    """ To extract ID for *large* Illumina fastq file.

    Note: for paired-end reads by illumina, the header lines for the same pair of reads are different at the end (/1, /2)
          header_line[:-3] is used to get the common part of the header line

    according to http://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers, Sequences from the Illumina software use a systematic identifier

        @HWUSI-EAS100R:6:73:941:1973#0/1
        -------------------------------------------------------------
        HWUSI-EAS100R   the unique instrument name
        6               flowcell lane
        73              tile number within the flowcell lane
        941             'x'-coordinate of the cluster within the tile
        1973            'y'-coordinate of the cluster within the tile
        #0              index number for a multiplexed sample (0 for no indexing). \
                        Versions of the Illumina pipeline since 1.4 appear to use #NNNNNN instead \
                        of #0 for the multiplex ID, where NNNNNN is the sequence of the multiplex tag.
        /1              the member of a pair, /1 or /2 (paired-end or mate-pair reads only)
        -------------------------------------------------------------

    example:
        "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/1" --> "6:1101:2436:2161"
        "@FCD0R5AACXX:6:1101:2436:2161#CGATGTAT/2" --> "6:1101:2436:2161"

     Note:
     With Casava 1.8 the format of the '@' line has changed:
            @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG
    """

    # return header_line.split('#')[0]
    return header_line[:-3].partition(':')[-1].partition('#')[0]
header2ID_illumina.p1 = re.compile(r'^@[^:]+(:[0-9]+){4}#[ATGC]+/1$')
header2ID_illumina.p2 = re.compile(r'^@[^:]+(:[0-9]+){4}#[ATGC]+/2$')

def header2ID_illumina_new(header_line):
    """ To extract ID for *large* Illumina fastq file.

    Note: for paired-end reads by illumina(WUxi Pharm), the header lines for the same pair of reads are different at the end (/1, /2)
          header_line[:-3] is used to get the common part of the header line

    according to http://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers, With Casava 1.8 the format of the '@' line has changed:

            @EAS139:136:FC706VJ:2:2104:15343:197393 1:Y:18:ATCACG
            -------------------------------------------------------------
            EAS139      the unique instrument name
            136         the run id
            FC706VJ     the flowcell id
            2           flowcell lane
            2104        tile number within the flowcell lane
            15343       'x'-coordinate of the cluster within the tile
            197393      'y'-coordinate of the cluster within the tile
            1           the member of a pair, 1 or 2 (paired-end or mate-pair reads only)
            Y           Y if the read is filtered, N otherwise
            18          0 when none of the control bits are on, otherwise it is an even number
            ATCACG      index sequence
            -------------------------------------------------------------

    example:
        "@HISEQ02:4:C4LU6ACXX:8:1101:1249:2231 1:N:0:ATCACG" --> 4:C4LU6ACXX:8:1101:1249:2231
        "@HISEQ02:4:C4LU6ACXX:8:1101:1249:2231 2:N:0:ATCACG" --> 4:C4LU6ACXX:8:1101:1249:2231

    Note: this function is actually the same with header2ID_AROS.
    """

    # return header_line.split('#')[0]
    return header_line.split(' ')[0].partition(':')[-1]
header2ID_illumina_new.p1 = re.compile(r'^@[^:]+(:[0-9A-Z]+){6} 1:[YN]:[0-9]+:[ATGC]+$')
header2ID_illumina_new.p2 = re.compile(r'^@[^:]+(:[0-9A-Z]+){6} 2:[YN]:[0-9]+:[ATGC]+$')

def header2ID_AROS(header_line):
    """ To extract ID for aros fastq file used by CSrK.

    example: 
        "@HWI-ST301L:301:C1BRBACXX:3:1101:1765:2207 1:N:0:CGATGT" --> "@HWI-ST301L:301:C1BRBACXX:3:1101:1765:2207"
        "@HWI-ST301L:301:C1BRBACXX:3:1101:1765:2207 2:N:0:CGATGT" --> "@HWI-ST301L:301:C1BRBACXX:3:1101:1765:2207"
    """

    return header_line.split()[0]
header2ID_AROS.p1 = re.compile(r'^@[^:]+(:[^:]+){6} 1(:[^:]+){3}$')
header2ID_AROS.p2 = re.compile(r'^@[^:]+(:[^:]+){6} 2(:[^:]+){3}$')

def header2ID_custom(header_line):
    """ put your own function to extract ID from header line here """

    raise Exception("Function header2ID_custom not implemented.")

def guess_header_format(fq1, fq2):
    """ To *guess* the format of the header line and return a function to extract ID.

    Note: in most cases, you should not use this function as this function is very conservative and not efficient!!

    Input:
        fq1: fqstq file one
        fq2: fqstq file two

    Return:
        a function like header2ID_*

    """

    # look at the first 100 records
    records1 = islice(fastq_iter(fq1), 100)
    records2 = islice(fastq_iter(fq2), 100)

    if (not records1) or (not records2):
        stderr_write("[%s] Input fastq records are blank! Will exit.\n" % date())
        sys.exit()

    # first check if currently available read ID extraction functions could work with the data.

    # currently only header2ID_illumina and header2ID_AROS are available for checking. header2ID_illumina_old is deprecated. 
    for k,v in globals().items():    # iter over all header2ID_* functions, which already have .p1 and .p2 attributes matching headers of read1/2
        if k.startswith('header2ID_') and callable(v) and hasattr(v, 'p1') and hasattr(v, 'p2'):
            stderr_write("[%s] checking %s ... " % (date(), k))
            p1, p2 = v.p1, v.p2
            if ((all(re.search(p1, rec[0]) for rec in records1) and all(re.search(p2, rec[0]) for rec in records2)) or 
                (all(re.search(p2, rec[0]) for rec in records1) and all(re.search(p1, rec[0]) for rec in records2))):
                stderr_write("Yes!\n")
                return v
            else:
                stderr_write("No.\n")

    stderr_write("[%s] couldn't recognize header format, will try to infer.\n" % date())
    stderr_write("[%s] YOU SHOULD PAY ATTENTION TO THE HEADER FORMAT.\n" % date())

    # if currently available header2ID_* functions don't work, will try to infer the format of header line:
    return infer_header_format(fq1, fq2)

def infer_header_format(fq1, fq2):
    """
    infer the header line format and return a function to extract read ID from header line
    This function is (very likely to be) reliable but conservative, and might be slow. Use with caution.
    """

    # look at the first 100 records
    records1 = islice(fastq_iter(fq1), 100)
    records2 = islice(fastq_iter(fq2), 100)

    # '@HWI-ST301L:301:C1BRBACXX:3:1101:1765:2207 1:N:0:CGATGT' --> 
    #       ['@HWI-ST', '301', 'L:', '301', ':C', '1', 'BRBACXX:', '3', ':', '1101', ':', '1765', ':', '2207', ' ', '1', ':N:', '0', ':CGATGT']
    p = re.compile('([0-9]+)')
    headers1 = [re.split(p, rec[0]) for rec in records1]
    headers2 = [re.split(p, rec[0]) for rec in records2]

    # get length of items in each header line
    L1, L2 = len(headers1[0]), len(headers2[0])
    # check if all headers have same number of items
    if not all(len(row) == L1 for row in headers1):
        raise Exception("header lines in %s have different length" % fq1)
    if not all(len(row) == L2 for row in headers2):
        raise Exception("header lines in %s have different length" % fq1)
    if L1 != L2:
        raise Exception("header lines in %s and %s have different length" % (fq1, fq2))

    idx1 = [ i for i in range(L1) if (all(row[i] == '1' for row in headers1) or all(row[i] == '2' for row in headers1))]
    idx2 = [ i for i in range(L2) if (all(row[i] == '1' for row in headers2) or all(row[i] == '2' for row in headers2))]

    if idx1 != idx2:
        raise Exception("header lines in %s and %s have different pattern" % (fq1, fq2))

    # if int(headers1[0][i]) * int(headers2[0][i]) == 2 means headers1[0][i] and headers2[0][i] == (1,2) or (2,1)
    idx = [i for i in idx1 if int(headers1[0][i]) * int(headers2[0][i]) == 2]
    if len(idx) != 1:
        raise Exception("Couldn't guess the format of the header line. You'd better modify the script to write your own header2ID_custom function")

    # we have to remove the indicator "1" or "2" in the final result
    idx_rm = idx[0]

    return lambda line: "".join(item for (i, item) in enumerate(re.split(p, line)) if i != idx_rm)

#-------------------------------------------------

def intersect_fastq(fq1, fq2, header2ID=header2ID_illumina, output_dir=None, prefix=None, force_overwrite=False, gz=False, multi_processing=False):
    """ To get intersection of a pair of fastq files

    fq1, fq2: fastq files of the same pair
    header2ID: function to extract read unique ID from header line
    output_dir: output dir
    """

    N_size = 500000   # number of records read each time

    #
    # -------prepare output directories and files------
    dirname_1, basename_1 = path.split(path.abspath(path.expanduser(fq1)))
    dirname_2, basename_2 = path.split(path.abspath(path.expanduser(fq2)))

    prefix_1, num_1, ext_1, gz_1 = re.findall(r'(.+)(_[12])(.fq|.fastq)(.gz)?$', basename_1)[0]
    prefix_2, num_2, ext_2, gz_2 = re.findall(r'(.+)(_[12])(.fq|.fastq)(.gz)?$', basename_2)[0]

    # take care of output_dir
    if not output_dir:
        if dirname_1 == dirname_2:
            output_dir = dirname_1
        else:
            raise Exception("--output_dir OUTPUT_DIR is not provided, but fq1 and fq2 are from different directories.\n")
    else:
        output_dir = path.abspath(path.expanduser(output_dir)) # if output_dir is provided by user, it may have "~" in the path

    # take care of prefix
    if prefix:   # the prefix provided by user might be a path
        prefix_head, prefix_tail = path.split(path.expanduser(prefix))
        if path.isabs(prefix_head):         # if prefix is absolute path, output_dir will be ignored.
            output_dir = prefix_head
        else:                               # if prefix is relative path, prefix_head will be appended to output_dir
            output_dir = path.join(output_dir, prefix_head)
        prefix = prefix_tail
    else:   #if prefix is not provided by user
        # prefix = path.commonprefix([basename_1, basename_2])
        if prefix_1 == prefix_2:
            prefix = prefix_1 + ".comm"
        else:
            raise Exception("--prefix PREFIX is not provided, but fq1 and fq2 have different prefixes.\n")

    if not path.exists(output_dir):         # create output_dir if not exists
        makedirs(output_dir)

    # output files
    fq1_fout = path.join(output_dir, "%s%s%s" % (prefix, num_1, ext_1))
    fq2_fout = path.join(output_dir, "%s%s%s" % (prefix, num_2, ext_2))
    if gz: # if output should be in .gz format
        fq1_fout += ".gz"          # add .gz to output file name if needed
        fq2_fout += ".gz"          # add .gz to output file name if needed

    # check if output already exists
    if path.exists(fq1_fout) or path.exists(fq2_fout):
        if not force_overwrite:
            stderr_write("[%s] %s and/or %s already exists, will exit and not overwrite it.\n" % (date(), fq1_fout, fq2_fout))
            return
        else:
            stderr_write("[%s] %s and/or %s already exists, but will overwrite it.\n" % (date(), fq1_fout, fq2_fout))
    stderr_write("[%s] Will output to %s and %s.\n" % (date(), fq1_fout, fq2_fout))

    if gz:
        fq1_comm = gzip.open(fq1_fout, 'wb')
        fq2_comm = gzip.open(fq2_fout, 'wb')
    else:
        fq1_comm = open(fq1_fout, 'w')
        fq2_comm = open(fq2_fout, 'w')

    #
    # ----------------do the job------------------------------

    # prepare sqlite3
    with sqlite3.connect(":memory:") as c:
        c.execute("PRAGMA  synchronous = OFF;")
        c.execute("PRAGMA  page_size = 4096;")
        c.execute("PRAGMA  cache_size = 1000000;")
        c.execute("PRAGMA  read_uncommitted=1;")
        c.execute("PRAGMA  locking_mode = EXCLUSIVE;")
        c.execute("PRAGMA  journal_mode = OFF;")

        c.executescript("""create table if not exists fq1 (id TEXT PRIMARY KEY, fq TEXT);
                           create table if not exists fq2 (id TEXT PRIMARY KEY, fq TEXT); """)

        def p_write(fobj, data):
            """ write "".join(data) into fobj """
            fobj.write("".join(data))

        fq1_iter, fq2_iter  = fastq_iter(fq1), fastq_iter(fq2)
        N1 = N2 = NC = 0
        while True:

            stderr_write("[%s] start to read\n" % (date()))
            fq1_data = [(header2ID(record[0]), "".join(record)) for record in islice(fq1_iter, N_size)]
            fq2_data = [(header2ID(record[0]), "".join(record)) for record in islice(fq2_iter, N_size)]
            stderr_write("[%s] end to read\n" % (date()))

            if (not fq1_data) and (not fq2_data):
                stderr_write("[%s] done\n" % date())
                break
            if fq1_data:
                N1 += len(fq1_data)/1e6
                stderr_write("[%s] have read %.2f M records for fq1\n" % (date(), N1))
                c.executemany("insert into fq1(id, fq) values (?, ?)", fq1_data)
            if fq2_data:
                N2 += len(fq2_data)/1e6
                stderr_write("[%s] have read %.2f M records for fq2\n" % (date(), N2))
                c.executemany("insert into fq2(id, fq) values (?, ?)", fq2_data)

            comm_data = c.execute("""select fq1.rowid, fq2.rowid, fq1.fq, fq2.fq from fq1, fq2 where fq1.id == fq2.id;""").fetchall()
            if comm_data:
                stderr_write("[%s] %.2fM + %d records to output... \n" % (date(), NC, len(comm_data)))
                NC += len(comm_data)/1e6
                rowids_1, rowids_2, comm_1, comm_2 = izip(*comm_data)

                stderr_write("[%s] start to write\n" % (date()))
                if not multi_processing:
                    fq1_comm.write("".join(comm_1)) # fq1
                    fq2_comm.write("".join(comm_2)) # fq2
                else:
                    w_procs = [Process(target=p_write, args=(fobj, data)) for fobj, data in ((fq1_comm, comm_1), (fq2_comm, comm_2))]
                    for job in w_procs: job.start()
                    for job in w_procs: job.join()
                stderr_write("[%s] end to write\n" % (date()))

                c.executescript("""delete from fq1 where rowid <= %d;
                                   delete from fq2 where rowid <= %d;""" % (rowids_1[-1], rowids_2[-1]))
                stderr_write("[%s] end to delete\n" % (date()))

#-------------------------------------------------
# names for all header2ID_* functions
header_formats = ['guess', 'infer']   # there is no 'header2ID_guess', 'guess' will be treated separately when processing arguments
for k,v in locals().items():
    if k.startswith('header2ID_') and callable(v):
        header_formats.append(k[10:])  # remove 'header2ID_' from the function name

#=================================================
if __name__ == '__main__':

    import argparse

    parser = argparse.ArgumentParser(description='To get intersection of a pair of fastq files that have been filtered separately.')
    parser.add_argument('-f', '--header_format', default="guess", choices=header_formats, help="header line format of the fastq file, default is 'guess', and it should work for most cases.")
    parser.add_argument('-o', '--output_dir', default=None, metavar="output_dir", help="directory of the output files")
    parser.add_argument('-p', '--prefix', default=None, metavar="prefix", help="prefix of the output files. If prefix is relative path, it will be joined to output_dir; If prefix is absolute path, output_dir will be ignored.")
    parser.add_argument('-F', '--force_overwrite', default=False, action="store_true", help="force overwrite if output file already exists. Default is 'False'.")
    parser.add_argument('-z', '--gzip', default=True, dest="gz", action="store_true", help="output file in gzip format. Default is 'False'.")
    parser.add_argument('-m', '--multi_processing', default=False, dest="multi_processing", action="store_true", help="to use multiprocessing to speed up the writing of output files. Default if 'False'. Note: 1) if you submit job with PBS, you may need to assign ncpus=3 for every job; 2) multi_processing could only help if the output is in .gzip format.")
    parser.add_argument('file_list', nargs=2, metavar="fq_file", help="fastq1 and fastq2 (could be gzip file)")

    args = parser.parse_args()
    if args.header_format == "guess":    # guess the format of header line
        header2ID = guess_header_format(*args.file_list)
    elif args.header_format == "infer":
        header2ID = infer_header_format(*args.file_list)
    elif args.header_format in header_formats:
        header2ID = locals()['header2ID_' + args.header_format]
    else:
        raise Exception("Don't recognize '--header_format %s', will now exit." % (args.header_format))

    intersect_fastq(args.file_list[0], args.file_list[1], header2ID=header2ID, output_dir=args.output_dir, prefix=args.prefix, force_overwrite=args.force_overwrite, gz=args.gz, multi_processing=args.multi_processing)
